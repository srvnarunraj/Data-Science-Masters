{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f6e909-5321-4151-8547-2b551658beab",
   "metadata": {},
   "source": [
    "## Q1. Here's a possible implementation of the pipeline that automates feature engineering and handles missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab7105-6929-444a-8377-15981408e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assume X is the input data (features) and y is the target variable\n",
    "\n",
    "# Step 1: Automated feature selection\n",
    "# use SelectKBest with f_classif scoring function to select the top 10 features\n",
    "kbest = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Step 2: Numerical pipeline\n",
    "# impute missing values with mean and scale numerical columns\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 3: Categorical pipeline\n",
    "# impute missing values with most frequent value and one-hot encode categorical columns\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Step 4: Combine numerical and categorical pipelines using ColumnTransformer\n",
    "# use kbest to select the top 10 features\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, ['num_col_1', 'num_col_2', ...]),\n",
    "    ('cat', cat_pipeline, ['cat_col_1', 'cat_col_2', ...]),\n",
    "    ('kbest', kbest, ['num_col_1', 'num_col_2', ..., 'cat_col_1', 'cat_col_2', ...])\n",
    "])\n",
    "\n",
    "# Step 5: Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 6: Combine everything into a single pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "# Step 7: Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Train and evaluate the model\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Step 9: Interpretation and possible improvements\n",
    "# interpret the results by analyzing accuracy, confusion matrix, feature importance, etc.\n",
    "# possible improvements include trying different feature selection methods, imputation strategies, scaling methods, classifiers, hyperparameters, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c40d1d-c297-4ee5-8004-e8aebcdf47b3",
   "metadata": {},
   "source": [
    "## Q2. Here's a possible implementation of the pipeline that combines a Random Forest Classifier and a Logistic Regression Classifier using a Voting Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e7a8-3c19-4527-870c-b7479f838e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assume X is the input data (features) and y is the target variable\n",
    "\n",
    "# Step 1: Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 2: Logistic Regression Classifier\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# Step 3: Voting Classifier\n",
    "# use \"soft\" voting to take into account the predicted probabilities\n",
    "voting = VotingClassifier(estimators=[('rf', rf), ('lr', lr)], voting='soft')\n",
    "\n",
    "# Step 4: Combine everything into a single pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classifier', voting)\n",
    "])\n",
    "\n",
    "# Step 5: Split data into train and test sets\n",
    "X_train, X_test, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4accdc7-44c6-4f01-969a-89a41e1d1a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
