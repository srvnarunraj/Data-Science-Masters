{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b683f74-e790-4415-b727-d4726fa33568",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Bagging reduces overfitting in decision trees by creating multiple bootstrapped samples from the training data and training a decision tree on each sample. Each tree will have slightly different splits and predictions due to the variation in the samples. When predicting on new data, the ensemble of trees makes predictions by averaging or taking a majority vote, which reduces the variance of the predictions and makes the model less prone to overfitting.\n",
    "\n",
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    "The advantages of using different types of base learners in bagging are that it can improve the overall accuracy of the ensemble, and it can reduce the risk of relying too heavily on one type of model. However, the disadvantages are that it can increase the complexity of the ensemble, and it may be difficult to combine models with different outputs or interpretations.\n",
    "\n",
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    "The choice of base learner can affect the bias-variance tradeoff in bagging. A high-bias model, such as a linear regression model, may not benefit as much from bagging as a high-variance model, such as a decision tree. Using a high-bias model as the base learner may result in a lower overall variance but also a higher overall bias in the ensemble, while using a high-variance model as the base learner may result in a lower overall bias but also a higher overall variance in the ensemble.\n",
    "\n",
    "## Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    "Bagging can be used for both classification and regression tasks. In classification tasks, the base learner is typically a decision tree or a random forest, and the ensemble makes predictions based on the majority vote of the individual models. In regression tasks, the base learner is typically a regression tree or a boosted tree, and the ensemble makes predictions based on the average of the individual models.\n",
    "\n",
    "\n",
    "## Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "\n",
    "The ensemble size in bagging can affect the accuracy and computational efficiency of the model. Increasing the ensemble size can improve the accuracy of the model up to a certain point, after which the improvement becomes marginal or even negligible. The optimal ensemble size depends on the complexity of the problem, the size of the training data, and the computational resources available.\n",
    "\n",
    "## Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "\n",
    "A real-world application of bagging in machine learning is in the field of bioinformatics, where it is used for gene expression analysis and protein structure prediction. For example, bagging has been used to identify gene regulatory networks from gene expression data, and to predict the functional properties of proteins based on their amino acid sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
