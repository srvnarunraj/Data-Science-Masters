{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378da670-850f-4f8a-9ee1-23b227e765ba",
   "metadata": {},
   "source": [
    "### Q1Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46379b8c-1359-4b21-9eea-7421d9042cf9",
   "metadata": {},
   "source": [
    "* R-squared is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variables in a linear regression model. \n",
    "* It ranges from 0 to 1, with higher values indicating a better fit between the model and the data.\n",
    "* R-squared is calculated as 1 - (SSres / SStot), where SSres is the sum of squared residuals and SStot is the total sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0571f-19fc-4340-8021-1bb17ebcdcba",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0f9a1-b255-4f0c-b896-81ed53c67b01",
   "metadata": {},
   "source": [
    "* Adjusted R-squared is a modified version of R-squared that takes into account the number of predictor variables in the model.\n",
    "* It is calculated as 1 - ((1 - R-squared) * (n - 1) / (n - p - 1)), where n is the sample size and p is the number of predictor variables.\n",
    "* Adjusted R-squared penalizes the inclusion of irrelevant predictor variables in the model, making it a more appropriate metric when comparing models with different numbers of predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb9d1f-cd33-49b9-923d-b07f647b1994",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce138222-d1c2-4ab7-acd5-65301d659ced",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of predictor variables, as it takes into account the penalty for including additional variables that may not be relevant to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cde887-493e-44b3-8ab6-5085db3a0bd5",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c37287-bfa8-4a9d-af79-3d79bb8aeaea",
   "metadata": {},
   "source": [
    "* RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of regression models.\n",
    "* RMSE is calculated as the square root of the average squared difference between the predicted and actual values, \n",
    "* MSE is calculated as the average squared difference between the predicted and actual values\n",
    "* MAE is calculated as the average absolute difference between the predicted and actual values. \n",
    "\n",
    "These metrics represent the overall error of the model in predicting the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbb164-4575-423c-84dd-79b1ee6b4a28",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73ebd2-771b-44a2-b311-4e1874cf212d",
   "metadata": {},
   "source": [
    "* The advantages of RMSE, MSE, and MAE are that they provide a quantitative measure of the performance of the model and can be easily interpreted.\n",
    "* The disadvantage is that they do not take into account the specific context of the problem, such as the cost of false positives or false negatives. Additionally, RMSE and MSE are sensitive to outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961348af-b75e-4eb7-80bc-7d3abfa67a46",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c7598-d4c3-400e-9163-791d1200f251",
   "metadata": {},
   "source": [
    "* Lasso regularization is a technique used in linear regression to penalize the inclusion of irrelevant predictor variables by adding a penalty term to the cost function that is proportional to the absolute value of the coefficients.\n",
    "* It differs from Ridge regularization in that it can lead to sparse models where some of the coefficients are set to zero.\n",
    "* Lasso regularization is more appropriate when there is reason to believe that only a subset of the predictor variables are relevant to the mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7917d-2ac7-4dd7-a2aa-c19e9e39fa0d",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bb32f-1143-4483-ac78-284617bd1e36",
   "metadata": {},
   "source": [
    "* Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the cost function that discourages the inclusion of irrelevant predictor variables or large coefficients.\n",
    "* For example, in Lasso regularization, the penalty term encourages some of the coefficients to be set to zero, resulting in a simpler and more interpretable model that is less likely to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d165e7-1608-4c15-8034-8399ef18c282",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best  choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe816e8-2f9c-4f3a-a87e-247fb7d5c46a",
   "metadata": {},
   "source": [
    "* The limitations of regularized linear models are that they can be sensitive to the choice of regularization parameter and the specific context of the problem.\n",
    "* Additionally, they assume a linear relationship between the predictor variables and the response variable, which may not always be the case in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a205d-f087-43dd-8c43-4475d92beace",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290e881-fbeb-44d0-90da-b46b62d62946",
   "metadata": {},
   "source": [
    "* In this case, the choice of metric depends on the specific context of the problem. RMSE and MAE represent different aspects of the error in the model, with RMSE being more sensitive to outliers and MAE being more robust to them.\n",
    "* Depending on the specific cost of false positives and false negatives in the problem, one metric may be more appropriate than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01d804-e92f-44db-a276-384f78794d85",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization.\n",
    "Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc1905-fb9d-4392-9d94-7652599d3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To choose between Model A and Model B, we need to evaluate their performance using appropriate evaluation metrics.\n",
    "# We can use cross-validation to get an estimate of their performance on unseen data.\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the models\n",
    "model_a = Ridge(alpha=0.1)\n",
    "model_b = Lasso(alpha=0.5)\n",
    "\n",
    "# Load the data\n",
    "X, y = load_data()\n",
    "\n",
    "# Evaluate the models using cross-validation and the mean squared error metric\n",
    "mse_a = np.mean(cross_val_score(model_a, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "mse_b = np.mean(cross_val_score(model_b, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Choose the model with the lower MSE\n",
    "if mse_a < mse_b:\n",
    "    print(\"Model A is the better performer.\")\n",
    "else:\n",
    "    print(\"Model B is the better performer.\")\n",
    "\n",
    "# Ridge regularization tends to shrink the coefficients towards zero, but it does not set them exactly to zero.\n",
    "# Lasso regularization, on the other hand, can set some of the coefficients exactly to zero, which can lead to\n",
    "# feature selection and simpler models. However, Lasso tends to be more sensitive to outliers and can be unstable\n",
    "# when the number of features is larger than the number of observations. Therefore, the choice of regularization\n",
    "# method depends on the specific problem and the trade-offs between model complexity and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7226c-b1d2-489b-97e5-0a60dbf1f2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
