{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ce3b73-838f-416f-b725-a271a7f84f2b",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88b410-3d8e-457d-a96a-82e30c41cbd5",
   "metadata": {},
   "source": [
    "* Simple linear regression involves predicting a response variable using a single predictor variable\n",
    "* **Example of simple linear regression :** Predicting a person's salary based on their years of experience\n",
    "* Multiple linear regression involves predicting the response variable using multiple predictor variables.\n",
    "* **Example of multiple linear regression :** Predicting a person's salary based on their years of experience, education level, and job title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef204e1f-2c67-47d8-a0a6-defe6f40e141",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe5899-38d3-4f0e-a214-33ccb24aed95",
   "metadata": {},
   "source": [
    " The assumptions of linear regression include linearity, independence, normality, homoscedasticity, and absence of multicollinearity. We can check whether these assumptions hold in a given dataset by examining residual plots, Q-Q plots, and correlation matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da6ba5-0d57-488a-9025-9cdf06fc2be9",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c6b77-47c6-4c6c-9ac2-13b2bd47604f",
   "metadata": {},
   "source": [
    "* The slope in a linear regression model represents the change in the response variable associated with a one-unit increase in the predictor variable.\n",
    "* The intercept represents the value of the response variable when the predictor variable is equal to zero.\n",
    "* **Example :** In a simple linear regression model predicting salary based on years of experience, the slope would represent the increase in salary associated with each additional year of experience, and the intercept would represent the starting salary for someone with zero years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a78c8-3a25-4ead-99bf-7875e2cee7c5",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8262d-c1c4-4ae5-ad35-5bbc15c93777",
   "metadata": {},
   "source": [
    "* Gradient descent is an optimization algorithm used in machine learning to find the values of the parameters that minimize the cost function.\n",
    "* It involves iteratively updating the values of the parameters in the direction of the negative gradient of the cost function until convergence is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75a4d0-1ec7-4822-a5eb-93c46af9d36c",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40689b4a-43e7-4b3f-ad41-7f295d19c271",
   "metadata": {},
   "source": [
    "Multiple linear regression is a regression model that includes multiple predictor variables, whereas simple linear regression involves only one predictor variable. The formula for multiple linear regression is y = b0 + b1x1 + b2x2 + ... + bn*xn, where y is the response variable, x1, x2, ..., xn are the predictor variables, b0 is the intercept, and b1, b2, ..., bn are the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4abab3-1ee6-4a59-97ee-9439d513e99a",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353e861-6377-4567-95aa-a73ac801b223",
   "metadata": {},
   "source": [
    "* Multicollinearity in multiple linear regression occurs when two or more predictor variables are highly correlated with each other.\n",
    "* This can lead to inflated standard errors and inaccurate coefficient estimates.\n",
    "* Multicollinearity can be detected by examining the correlation matrix and checking for high correlations between predictor variables. \n",
    "* It can be addressed by removing one of the correlated variables or by using regularization techniques such as ridge regression or lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc1a49-408c-470d-a20f-15e82de98805",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0bb05-e87d-4085-96c2-0fa77f66041f",
   "metadata": {},
   "source": [
    "Polynomial regression is a regression model that includes polynomial terms (e.g., x^2, x^3) in addition to linear terms. It is used when the relationship between the response variable and predictor variable is not linear. Polynomial regression is different from linear regression in that it allows for nonlinear relationships between the response variable and predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19f615-5456-4384-9af1-39e0efaba91c",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90349a36-07b9-44c6-b7cb-c78a0da163fc",
   "metadata": {},
   "source": [
    "* The advantage of polynomial regression is that it can model nonlinear relationships between the response variable and predictor variable.\n",
    "* The disadvantage is that it can be prone to overfitting and can be computationally expensive when higher-order polynomial terms are used.\n",
    "* Polynomial regression is preferred when there is evidence of a nonlinear relationship between the response variable and predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca6436-eefe-42dd-9aba-ad33fd2752b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
