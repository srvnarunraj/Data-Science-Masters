{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb682484-90e2-498f-84c1-ef06f9c4cc01",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression is a type of linear regression that is used for feature selection and regularization. It is similar to Ridge Regression in that it adds a penalty term to the loss function, but the difference is that Lasso uses L1 regularization, which can shrink some coefficients to zero and perform feature selection. This means that Lasso Regression can automatically select the most important features and exclude the less important ones, resulting in a simpler and more interpretable model.\n",
    "\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is that it can automatically select the most important features and exclude the less important ones by shrinking some coefficients to zero. This results in a simpler and more interpretable model, and can also help to prevent overfitting by reducing the variance of the model.\n",
    "\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "The coefficients of a Lasso Regression model can be interpreted in the same way as those of a standard linear regression model. Each coefficient represents the change in the response variable that is associated with a one-unit change in the corresponding predictor variable, while holding all other variables constant. However, in Lasso Regression, some coefficients may be shrunk to zero, which means that the corresponding variables are not considered important in the model.\n",
    "\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter (lambda), which controls the strength of the L1 penalty term that is added to the loss function. A higher value of lambda will result in more coefficients being shrunk to zero, which can improve the model's interpretability and reduce overfitting, but may also increase bias and reduce the model's performance on the test set. On the other hand, a lower value of lambda will result in fewer coefficients being shrunk to zero, which can increase the model's complexity and risk overfitting, but may also improve its performance on the test set.\n",
    "\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression is a linear regression technique and is therefore not suitable for non-linear regression problems. However, it can be used in combination with non-linear transformations of the input variables, such as polynomial or interaction terms, to model non-linear relationships between the variables.\n",
    "\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression is the type of regularization used. Ridge Regression uses L2 regularization, which adds a penalty term proportional to the square of the coefficients to the loss function, while Lasso Regression uses L1 regularization, which adds a penalty term proportional to the absolute value of the coefficients. This results in different shrinkage patterns, with Ridge Regression tending to shrink all coefficients towards zero but not necessarily to zero, while Lasso Regression tends to shrink some coefficients to exactly zero, resulting in feature selection.\n",
    "\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Lasso Regression can handle multicollinearity in the input features by shrinking some of the correlated features to zero, effectively selecting the most important one among them. However, this can also lead to instability and unpredictability in the model, especially when the correlations between the features are high. In such cases, Ridge Regression may be a better choice as it can shrink all correlated features together instead of selecting one over the others.\n",
    "\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "In Lasso Regression, the optimal value of the regularization parameter (lambda) is chosen using cross-validation. Cross-validation involves dividing the dataset into k equally sized folds, training the model on k-1 folds, and evaluating it on the remaining fold. This process is repeated k times, with each fold used exactly once as the validation data. The average error across all k trials is used as the estimate of the model's performance.\n",
    "\n",
    "To choose the optimal value of lambda, the cross-validation process is repeated for different values of lambda, and the value of lambda that minimizes the average error across all k trials is selected. This approach is known as k-fold cross-validation.\n",
    "\n",
    "Another approach for selecting the optimal value of lambda is to use the Lasso path. The Lasso path is a plot of the coefficients of the Lasso Regression model as a function of lambda. It can be used to visualize how the coefficients change as lambda is varied. The optimal value of lambda is chosen based on the point at which the test error is minimized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
